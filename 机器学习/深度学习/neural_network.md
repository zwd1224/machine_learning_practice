
**机器学习流程**：数据获取；特征工程；建立模型；评估应用；（偏人工）

**深度学习**: (更智能)  怎么去提取特征，不可解释，计算量大；

---

# 前向传播 ：
将输入数据逐层传递，计算神经网络的预测输出。

- 通过线性函数计算属于每个类别得分
$$  f = Wx  $$  
- 使用softmax函数归一化，计算损失值
$$ P(Y = k|X = x_i) = \frac{e^{s_k}}{\sum_j e^{s_j}} $$
- 计算损失值：
$$ L_i = -\log P(Y = y_i|X = x_i) $$
$$  L_i = \sum_{j\neq y_i} \max(0, s_j - s_{y_i} + 1)$$

- 加入正则化 R(W)

---
# 反向传播（BP）
反向传播（Backpropagation）是神经网络中通过链式法则计算损失函数对权重梯度的高效算法。其核心思想是**从输出层向输入层反向传播误差信号**，逐层计算梯度并更新参数。以下是原理和具体示例：


##### **反向传播原理**
1. **前向传播**：输入数据通过网络逐层计算，得到预测值和损失。
2. **损失计算**：通过损失函数（如均方误差、交叉熵）衡量预测值与真实值的差异。
3. **反向传播**：
   - 计算损失对输出层输出的梯度。
   - 从输出层到输入层，逐层计算梯度：
     - 梯度 = 上游梯度 × 当前层激活函数的导数 × 输入值。
   - 最终得到损失对所有权重 $ W $ 的梯度。
4. **参数更新**：使用梯度下降法更新权重。


##### **简单示例：单隐藏层网络**
假设一个神经网络结构如下：
- **输入层**：$ x_1 = 0.5, x_2 = 0.8 $
- **隐藏层**：2个神经元，激活函数为 Sigmoid。
- **输出层**：1个神经元，无激活函数（直接输出）。
- **权重**：随机初始化为 $ W_1 = \begin{bmatrix} 0.1 & 0.2 \\ 0.3 & 0.4 \end{bmatrix}, W_2 = \begin{bmatrix} 0.5 & 0.6 \end{bmatrix} $
- **真实值**：$ y = 1 $
- **损失函数**：均方误差 $ L = \frac{1}{2}(y - \text{输出})^2 $

##### **步骤1：前向传播**
1. 隐藏层输入：
   $$
   h_{\text{in}} = W_1 \cdot \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} 0.1 \times 0.5 + 0.2 \times 0.8 \\ 0.3 \times 0.5 + 0.4 \times 0.8 \end{bmatrix} = \begin{bmatrix} 0.21 \\ 0.47 \end{bmatrix}
   $$
2. 隐藏层输出（Sigmoid激活）：
   $$
   h_{\text{out}} = \sigma(h_{\text{in}}) = \begin{bmatrix} \frac{1}{1+e^{-0.21}} \\ \frac{1}{1+e^{-0.47}} \end{bmatrix} \approx \begin{bmatrix} 0.552 \\ 0.615 \end{bmatrix}
   $$
3. 输出层结果：
   $$
   o = W_2 \cdot h_{\text{out}} = 0.5 \times 0.552 + 0.6 \times 0.615 \approx 0.276 + 0.369 = 0.645
   $$
4. 计算损失：
   $$
   L = \frac{1}{2}(1 - 0.645)^2 \approx \frac{1}{2} \times 0.355^2 \approx 0.063
   $$

##### **步骤2：反向传播**
1. **计算输出层梯度**：
   $$
   \frac{\partial L}{\partial o} = -(y - o) = -(1 - 0.645) = -0.355
   $$
   - 损失对 $ W_2 $ 的梯度：
     $$
     \frac{\partial L}{\partial W_2} = \frac{\partial L}{\partial o} \cdot h_{\text{out}} = -0.355 \times \begin{bmatrix} 0.552 \\ 0.615 \end{bmatrix} \approx \begin{bmatrix} -0.196 \\ -0.218 \end{bmatrix}
     $$

2. **计算隐藏层梯度**：
   - 上游梯度传递到隐藏层：
     $$
     \frac{\partial L}{\partial h_{\text{out}}} = \frac{\partial L}{\partial o} \cdot W_2 = -0.355 \times \begin{bmatrix} 0.5 & 0.6 \end{bmatrix} = \begin{bmatrix} -0.1775 & -0.213 \end{bmatrix}
     $$
   - 考虑 Sigmoid 导数（$ \sigma'(h_{\text{in}}) = \sigma(h_{\text{in}}) \times (1 - \sigma(h_{\text{in}})) $）：
     $$
     \frac{\partial h_{\text{out}}}{\partial h_{\text{in}}} = \begin{bmatrix} 0.552 \times (1 - 0.552) \\ 0.615 \times (1 - 0.615) \end{bmatrix} \approx \begin{bmatrix} 0.247 \\ 0.237 \end{bmatrix}
     $$
   - 损失对 $ W_1 $ 的梯度：
     $$
     \frac{\partial L}{\partial W_1} = \frac{\partial L}{\partial h_{\text{out}}} \cdot \frac{\partial h_{\text{out}}}{\partial h_{\text{in}}} \cdot \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}^T
     $$
     以第一个隐藏神经元为例：
     $$
     \frac{\partial L}{\partial W_{1,11}} = (-0.1775) \times 0.247 \times 0.5 \approx -0.0219
     $$

##### **步骤3：更新权重**
使用梯度下降（学习率 $ \eta = 0.1 $）：
$$
W_2 \leftarrow W_2 - \eta \cdot \frac{\partial L}{\partial W_2} \approx \begin{bmatrix} 0.5 + 0.0196 \\ 0.6 + 0.0218 \end{bmatrix} = \begin{bmatrix} 0.5196 \\ 0.6218 \end{bmatrix}
$$



##### **核心思想总结**
1. **链式法则**：通过逐层分解梯度，将复杂计算拆解为局部导数相乘。
2. **反向计算**：从输出层开始，逐步计算各层权重梯度，避免重复计算。
3. **高效性**：与直接计算所有路径相比，反向传播的复杂度从指数级降低到线性级。

通过不断迭代前向传播和反向传播，神经网络能够逐步优化权重，提升预测精度。

---
# 激活函数

神经网络中的激活函数（Activation Function）是模型实现非线性的核心组件，它们决定了神经元是否被激活以及如何传递信号。以下是常用的激活函数及其特点：


#### **一、激活函数的作用**
1. **引入非线性**：使神经网络能够拟合复杂函数（如 XOR 问题）。
2. **决定输出范围**：控制神经元的输出幅度（如概率值、归一化输出）。
3. **梯度传递**：通过导数决定反向传播时的梯度大小，影响参数更新。



#### **二、常用激活函数**

##### **1. Sigmoid（S型函数）**
- **公式**：  
  \[
  \sigma(x) = \frac{1}{1 + e^{-x}}
  \]
- **输出范围**：\( (0, 1) \)，适合概率输出（如二分类最后一层）。
- **问题**：  
  - **梯度消失**：当输入值过大或过小时，梯度接近零，导致参数无法更新。  
  - **非零中心化**：输出均值不为零，可能影响梯度下降效率。  
- **应用场景**：二分类输出层、传统神经网络（现少用于隐藏层）。

##### **2. Tanh（双曲正切函数）**
- **公式**：  
  \[
  \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
  \]
- **输出范围**：\( (-1, 1) \)，零中心化，梯度下降更稳定。
- **问题**：  
  - 梯度消失问题依然存在（但比 Sigmoid 轻微）。
- **应用场景**：RNN 的隐藏层、需要零中心化输出的场景。

##### **3. ReLU（Rectified Linear Unit，修正线性单元）**
- **公式**：  
  \[
  \text{ReLU}(x) = \max(0, x)
  \]
- **输出范围**：\( [0, +\infty) \)。
- **优点**：  
  - 计算高效，解决梯度消失问题（正区间梯度恒为 1）。  
  - 稀疏激活：负输入直接置零，减少参数依赖性。  
- **缺点**：  
  - **死亡 ReLU**：负输入时梯度为零，神经元永久失效。  
- **应用场景**：大多数深度神经网络的隐藏层（默认选择）。

##### **4. Leaky ReLU（带泄漏的 ReLU）**
- **公式**：  
  \[
  \text{LeakyReLU}(x) = 
  \begin{cases} 
  x & \text{if } x > 0 \\
  \alpha x & \text{if } x \le 0 
  \end{cases}
  \]  
  （\(\alpha\) 为小常数，如 0.01）
- **改进**：解决死亡 ReLU 问题，负区间保留微小梯度。
- **应用场景**：需要缓解死亡神经元的场景。

##### **5. Parametric ReLU（PReLU，参数化 ReLU）**
- **公式**：类似 Leaky ReLU，但 \(\alpha\) 作为可学习参数。
- **优点**：自适应调整负区间斜率，提升模型表达能力。
- **应用场景**：复杂任务（如图像分类）。

##### **6. ELU（Exponential Linear Unit，指数线性单元）**
- **公式**：  
  \[
  \text{ELU}(x) = 
  \begin{cases} 
  x & \text{if } x > 0 \\
  \alpha (e^x - 1) & \text{if } x \le 0 
  \end{cases}
  \]
- **优点**：  
  - 负区间平滑过渡，缓解梯度消失和死亡神经元问题。  
  - 输出接近零均值。
- **缺点**：计算量稍大（涉及指数运算）。
- **应用场景**：对噪声敏感的任务（如语音处理）。

##### **7. Swish**
- **公式**：  
  \[
  \text{Swish}(x) = x \cdot \sigma(\beta x)
  \]  
  （\(\sigma\) 为 Sigmoid 函数，\(\beta\) 可调节，默认 1）
- **特点**：  
  - 平滑非单调，Google Brain 提出，实验性能优于 ReLU。  
  - 在深层网络中表现良好。
- **应用场景**：替代 ReLU 用于隐藏层。

##### **8. Softmax**
- **公式**：  
  \[
  \text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^n e^{x_j}}
  \]
- **输出范围**：\( (0, 1) \)，且和为 1，适合多分类概率输出。
- **应用场景**：多分类任务的输出层。

##### **9. GELU（Gaussian Error Linear Unit）**
- **公式**：  
  \[
  \text{GELU}(x) = x \cdot \Phi(x)
  \]  
  （\(\Phi(x)\) 是标准正态分布的累积分布函数）
- **特点**：  
  - 通过概率阈值决定是否激活，被用于 Transformer 和 BERT 等模型。  
- **近似计算**：  
  \[
  \text{GELU}(x) \approx 0.5x \left(1 + \tanh\left(\sqrt{\frac{2}{\pi}}(x + 0.044715x^3)\right)\right)
  \]


#### **三、选择激活函数的建议**
| **场景**               | **推荐激活函数**            |
|------------------------|---------------------------|
| **隐藏层（通用）**     | ReLU、Leaky ReLU、Swish   |
| **二分类输出层**       | Sigmoid                   |
| **多分类输出层**       | Softmax                   |
| **需要零均值输出**     | Tanh、ELU                 |
| **对抗死亡神经元**     | Leaky ReLU、ELU、PReLU    |
| **复杂任务（如 NLP）** | GELU、Swish               |



#### **四、激活函数对比表**
| **激活函数** | **非线性** | **梯度消失** | **死亡神经元** | **计算效率** | **输出范围**      |
|--------------|------------|--------------|----------------|--------------|-------------------|
| Sigmoid      | 饱和       | 严重         | 无             | 中等         | \( (0, 1) \)      |
| Tanh         | 饱和       | 中等         | 无             | 中等         | \( (-1, 1) \)     |
| ReLU         | 非饱和     | 无（正区间） | 有             | 高           | \( [0, +\infty) \)|
| Leaky ReLU   | 非饱和     | 无           | 缓解           | 高           | \( (-\infty, +\infty) \) |
| Swish        | 平滑       | 缓解         | 无             | 中等         | \( (-\infty, +\infty) \) |
| Softmax      | 饱和       | 无           | 无             | 中等         | 概率分布          |



#### **五、总结**
- **隐藏层首选**：ReLU 及其变种（Leaky ReLU、Swish）因计算高效且缓解梯度消失。  
- **输出层**：根据任务选择 Sigmoid（二分类）或 Softmax（多分类）。  
- **进阶场景**：GELU、Swish 在 Transformer 等复杂模型中表现优异。  
- **避免使用**：Sigmoid/Tanh 在深度网络的隐藏层中易导致梯度消失。


---

# 参数初始化：随机
` W = 0.01 *np.random.randn(D,H) ` 

---

# 正则化

**DROP-OUT**:随机丢弃神经元，迫使网络在训练时不过度依赖某些特定神经元，从而提升泛化能力。

**Dropout 的优缺点**
| **优点**                  | **缺点**                              |
|---------------------------|-------------------------------------|
| 显著减少过拟合            | 训练时间增加（每次迭代训练子网络）    |
| 无需额外调参（仅设置 \( p \)） | 可能降低模型容量（需增大网络宽度）    |
| 与大多数网络结构兼容        | 在小数据集上效果可能不明显            |



**与其他正则化方法的对比**
| **方法**        | **核心思想**                     | **适用场景**               |
|-----------------|----------------------------------|--------------------------|
| **Dropout**     | 随机丢弃神经元                   | 全连接层、防止特征依赖     |
| **L1/L2 正则化**| 惩罚权重的大小                   | 所有层、限制模型复杂度     |
| **数据增强**    | 增加训练数据多样性               | 输入层、图像/文本任务      |
| **Early Stop**  | 提前终止训练防止过拟合           | 通用、需验证集监控         |

---

